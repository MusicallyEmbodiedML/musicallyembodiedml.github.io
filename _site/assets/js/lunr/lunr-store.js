var store = [{
        "title": "Sussex AI Launch",
        "excerpt":"Before the project officially began,  Chris presented a prototype at the Sussex AI launch event.  It was a DiffLogic network running on an RP2040 microcontroller (hosted on a uSEQ module). The network listened to patterns of kickdrums and improvised drum patterns around this.        ","categories": ["event report"],
        "tags": ["event","presentation"],
        "url": "/~ck84/meml/event%20report/sussex-ai-launch/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Sussex AI Day 2024",
        "excerpt":"We presented the project at the Sussex AI day 2024.                ","categories": ["event report"],
        "tags": ["event","presentation"],
        "url": "/~ck84/meml/event%20report/sussex-ai-day/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Music RAI Workshop",
        "excerpt":"Andrea presented the project at the Music RAI workshop at QMUL, on ethical and responsible music making, and bias in AI music generation.   The organisers reported:      ‘We brought together over 100 people to form an interdisciplinary community of musicians, academics, and stakeholders to collaboratively identify the potential and challenges for using low-resource models and small datasets in musical practice. The workshop consisted of publicly streamed discussion panels, presentations of participants’ work, and brainstorming sessions on the future of AI and marginalised music. The event was followed by an evening reception featuring live performances sing AI and small datasets of music.’    Here are the slides from the presentation:        ","categories": ["event"],
        "tags": ["presentation"],
        "url": "/~ck84/meml/event/music-rai/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Joystick FM Synth Prototype",
        "excerpt":"We’ve been working on some tech groundwork, and making a prototype synth.  It uses machine learning to map from a 3D parameter space (a 3D joystick) into a bigger parameter space of an FM synth.  The prototype has controls for collecting training data, and for training the model. In the background, it uses an XMOS chip to run the neural network and the sound synthesis.     ","categories": ["demo"],
        "tags": ["prototype","xmos"],
        "url": "/~ck84/meml/demo/joystickproto/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Upcoming: PhD Course @ AAU Copenhagen",
        "excerpt":"We’re happy to announce that we’ll be running a PhD Course on Musically Embodied Machine Learning at AAU Copenhagen, in collaboration with Dan Overholt.  The course will take place from 15th-17th January 2025. We’ll be announcing more information soon, but please get in contact if you’re interested in attending.  ","categories": ["course","event"],
        "tags": ["phd"],
        "url": "/~ck84/meml/course/event/phd-course-announcement/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Further Experiments in Neural Mapping Design",
        "excerpt":"In the last month or so we’ve been doing some more experimentation with hardware instruments and mapping design using neural networks. In October, ahead of a visit to Machina Bristronica, Eric from Schlappi Engineering visited for a week long hack session, and we made a new instrument based on our FM synthesis joystick prototype, converted to use for sequencing in modular synthesis.     The sequencing engine runs a set of four euclidean sequencers, with four parameters (n, k, speed, offset) controlling the rhythm they generate.  This space of 16 parameters is mapped to the three dimensional joystick, via a neural network. Controls on the module allow the player to collect points in the parameter space and train the mappings in the neural network. This results in a joystick controlled techno!   Another experiment we’ve done is to take a similar system and use it to control a larger parameter space in a multifx engine.     This video shows us trying out the engine, with modular synth drums, e-drums and guitar.  There are four effects (flange, delay, ring mod and filter). These are mixed together through a mixing matrix, which means that each effect takes a mix of itself and other effects as the input, in addition to the live signal from the instrument.  The neural network controls all of the levels in the mixing matrix, in addition to the mix levels of the live input and the parameters of the effects.  This results different in effects fading in and out in different areas of the joystick, and makes some interesting and unexpected surprises.   These different instruments are creating a basis for us to run experiments in different ways to interact with neural networks as we train, modify and evolve them towards musical goals.  ","categories": ["demo"],
        "tags": ["prototype","xmos","multifx","modular","sequencing","dsp","neuralnetwork","mapping"],
        "url": "/~ck84/meml/demo/joystickexpts/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "MEML Workshop at CHIME 2024",
        "excerpt":"The Instruments   We have three instruments to use in the workshop, all with a similar interface.  Each instrument has a large-ish parameter space which is controlled by a 3D joystick, supported by other controls for engaging with machine learning models.   The instruments:      a multi-fx processor   a sequencer   an FM synthesiser   Connecting to the web interfaces   Each instrument is running a small web interface for extended options that control machine learning.   Documentation   Quick-start tutorial           Make sure you are in Inference mode: the LED on the unit should be Off, and the Web GUI should display the training radio button under the Training section.            Wiggle the joystick (and play a note at the same time, if required): you will hear the sound changing as you move the joystick. With the joystick, you can interact with the model and map the joystick’s coordinates to your desired sounds. The joystick moves around in two dimensions and you can twist its top knob too.            Switch the toggle on the unit up: the LED lights up, and you will be in Training mode.            Press the green button to the LEFT (“Draw”): the sounds mapped to the joystick will change randomly. This is the start of your training session.       Explore the new sounds with the joystick. Once you find one that you like, you have two options:            Press Draw again: the current sound will be manipulated with further random noise (see “Exploration Modes” in the Web GUI) so you can explore sounds that are similar to it, and fine-tune your base sound. By default, it will put the current sound at the centre of the joystick (“Pre-train” mode).       Press and hold the button on top of the joystick (“Save”): move the joystick to the desired position, and release the Save button. This creates a data point in the Dataset. After training, the selected sound will be in that position when you go back to Inference mode.           After a data point is saved into the Dataset, press “Draw” again: the network will be re-randomised, and you can repeat Step 5 to create more data points.        If you want to start the acquisition process again, the green button on the RIGHT (“Clear”) will clear the dataset. You can then start again from Step 3.            Experiment with populating a dataset with a few data points. Then, flick the toggle switch down to Inference. The Model will train for a few seconds, then display its final loss value on the Web interface.       Your Model is trained! Play around with it, find your saved sounds and explore the ones in between that the Model has generated. Then, start creating more than one Dataset and training more than one Model. Use different Exploration Modes too.   See the GUI guide for a detailed explanation.   Hardware interface elements   Joystick   The joystick lets you control the parameters of the instrument, by providing input data to the Model. The joystick operates in three dimensions: X-Y axis movements, and bi-directional twists of its knob. The button on top is the Save button (see below).   Training/Inference switch  This switch lets you go in and out of Training or Inference mode. In Training mode (green LED ON), you can acquire data and create a dataset. In Inference mode, the Model is trained for the set amount of Iterations, then the joystick guides you through the parameter space using the trained Model. This switch is equivalent to the Train/Acquire button in the Web GUI.   Draw button  The Draw button moves the training process by one step. At the start, it draws new random parameters. After that, it lets you “zoom” into the sound that the joystick points to when the Draw button is pressed, according to the selected Exploration mode (see Web GUI elements).   Clear button  This button clears the current Dataset and re-initialises the training sequence (Step 3 in the Quick Start guide). It is equivalent to the “Clear” button for Datasets in the Web GUI.  Web GUI elements   Datasets  Each radio button represents one Dataset. You can change between Datasets in Training mode. Datasets can be cleared at any time with the Clear button below. A Model cannot be trained on an empty dataset.   Models  Each radio button represents one Model. You can change between Models either in Training or in Inference mode. Models can be reset to random weights with the Reset button below.   Modes  There are two modes, “Training” and “Inference”. You can switch between them either by flicking the hardware switch on the unit, or by pressing the “Train”/”Acquire” button on the web interface. Every time you switch from Training to Inference mode, the current Model will get trained on the current Dataset. The green LED on the unit is turned on if you are in Training mode. If you switch in and out of Inference mode more than once on the same dataset, the network will train for longer on the same dataset.   Iterations  How long you want the network to train on your data? This slider sets the number of epochs from 1 to 2000. Loosely-trained models may also make inspiring sounds.   Exploration modes   Exploration modes are different ways to navigate random parameter spaces during training. Each of them is a different way to re-randomise the weights of a Model around a sound that you select with the joystick. The Exploration modes are:      NN Weights: Gaussian noise is summed to the weights of a neural network. Exploration Range controls the variance of the noise.   Pre-Train: The network is pre-trained so that a selected sound is moved to the centre of the joystick’s parameter space. Exploration Range controls the range around the centre that the joystick can reach.   Zoom: This mode “zooms in” around the sound that was selected by the joystick position. The Model’s weights are unaffected. Exploration Range sets the amount of zooming-in: less range means more zoom.   You can change Exploration modes and range at any point as you acquire more data in Training mode.   Troubleshooting   The Microsoft way  Unplug the USB-A cable going from the back of the unit to the power supply or USB hub, and plug it back in after a few seconds.   Web GUI is unresponsive  Have a laptop with a serial terminal ready (e.g. screen). Disconnect the main USB cable from the unit. Connect a MicroUSB cable to the Pi Pico contained in the unit. Open the device /dev/ttyACM0. Disconnect and reconnect the MicroUSB cable into the Pico. Then connect the main USB cable of the unit. Upon boot-up, the Pico should display two things:     the IP address that it uses for serving the Web GUI   A message acknowledging that the XMOS board has booted up and has supplied a status message   Check that the webpage refreshes correctly and the browser points at the right IP address, and is connected to the same network. Check that the XMOS board is powered up. None of the above works   Ask for help from the workshop organisers!  ","categories": ["workshop","conference"],
        "tags": ["workshop","documentation","instruments"],
        "url": "/~ck84/meml/workshop/conference/chime-workshop/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "MEML @ CHIME 2024",
        "excerpt":"At CHIME this year we ran a workshop and also shared some new instruments in the demo session. Here’s some documentation from the event.   ","categories": ["workshop","conference"],
        "tags": ["workshop","documentation","instruments","event report"],
        "url": "/~ck84/meml/workshop/conference/chime-report/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Presentation to the Music Technology Group at Universitat Pompeu Fabra, Barcelona",
        "excerpt":"Chris was visiting the Music Technology Group to examine Behzad Haki’s fascinating PhD research on generative realtime rhythmic accompaniment.  He also gave a presentation on the MEML project. Here are the slides from the presentation:        ","categories": ["presentation"],
        "tags": ["presentation"],
        "url": "/~ck84/meml/presentation/upfmtg/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
        "title": "Research Visit to the Intelligent Instruments Lab in Reykjavik",
        "excerpt":"Chris and Andrea recent spent a week in Iceland, visiting Thor Magnusson’s Intelligent Instruments Lab at the University of Reykjavik.  Following our participatory design method, we spent the week working with Tom Manoury, exploring the augmentation of a euphonium using our MEMLNaut technology. We also working with Stefanos Vasilakis, who used the PAF synthesis MEMLNaut patch with external synth hardware. The week culminated in performances at Mengi, with the addition a duet with Andrea and Betty Accorsi on sax, and a Brain Dead Ensemble style trio with Thor Magnusson playing the Threnoscope, Chris playing MEMLNaut/Xiasri and Betty playing sax.    ","categories": ["report"],
        "tags": ["report","performance"],
        "url": "/~ck84/meml/report/iil/",
        "teaser": "/~ck84/meml/assets/images/MEML%20Mastodon%20Profile%20Pic%20B.jpg"
      },{
    "title": "Page Not Found",
    "excerpt":"Sorry, but the page you were trying to view does not exist.  ","url": "/~ck84/meml/404.html"
  },{
    "title": "ML Approaches: Continuous Active Reinforcement Learning",
    "excerpt":"Introduction   CARL describes an interactive approach to reinforcement learning, where the player works in dialog with the machine, by indicating their preference for how the machine is acting.  This can be as simple as indicating ‘likes’ and ‘dislikes’ for how the machine responds to inputs in a particular way, or can expand to more complex approches where the player can engage in more depth with the interactive opportunities presented by the reinforcement learning algorithm.   CARL      Continuous, as in the process is an ongoing realtime negotiation between player and machine   Active, describing that the player is an active participant in guiding the reinforement learning machine.   Reinforcement   Learning decribing the machine learning approach   Reinforcement Learning Approach   DDPG: Actors and Critics   We follow an approach called Deep Deterministic Policy Gradient (DDPG).  In DDPG, there are two types of neural network interacting with each other, the actor and the critic.  The Actor network performs actions, such as mapping from the MEMLNaut joystick to synthesis parameters.  The Critic network estimates the value of the Actor’s actions.  The player provides feedback into this system by rewarding the Actor’s behaviour, both negatively and positvely.  DDPG optimises these networks so that the Actor moves towards the behaviours indicated by the player’s rewards, and the Critic increases the quality of its estimates of the Actor’s actions.   Replay Memory   Central to this system is the experience replay memory.  This is a store of snapshots in time when the player indicated a reward (positive or negative).  The replay memory is limited in size, and when it’s full the systemn will forget previous experiences.  As the system runs, minibatches of experiences are randomly selected from the replay memory and used to optimise the Actor and the Critic.   Interacting with DDPG   When we run DDPG in realtime with active guideance from the player (as opposed to e.g. offline in a simulator),  there are some interesting possibilities that open up for the player.  Beyond providing rewards, the player can also      Randomise the weights of the Actor network.   Randomise the weights of the Critic network.   Forget some or all of the experience replay memory   Vary the scaling of rewards   Quick-start guide      Plug the unit in, turn the joystick (or move the input sensors), hear the model make the first sounds.   Keep trying different positions. The model will slowly drift towards different sounds.   Start flicking Momentary A1 switch up or down. A flick up will give the model a positive reward, a flick downwards a negative one.   Keep experimenting with the inputs and keep giving feedback to the model.   If you feel like the model got stuck, flick Momentary B1 switch up to reset the actor.   Controls   Momentary switches   Momentary A1 (up): reset the actor - the actor restarts from a random position.   Momentary A2 (down): reset the critic - the critic forgets how to anticipate your taste.   Momentary B1 (up): positive reward for the model.   Momentary B2 (down): negative reward for the model.   Toggle switches   Toggle B1 (down): clear the replay memory (the “dataset” of the RL algorithm’s experiences).   Pots   X potentiometer: sets the frequency of optimisation - how often the critic affects the actor.   Y potentiometer: EXPERIMENTAL: reward scaling - associates a measure of how much you like-dislike an experience.   ","url": "/~ck84/meml/memlnaut/approaches/carl"
  },{
    "title": "MEMLNaut Firmware: PAF Synth with POPR",
    "excerpt":"Overview   This firmware is inspired by Mick Gordon’s extreme compressor.  It generates complex noise and distorted sound in reaction to joystick movement.  Note that this a sound generator rather than an effect.   Doom Compression   This describes a technique for using extremely high compression ratios.  Six sinewaves are detuned and mixed together with varying amplitudes.  They are put through a delay with variable feedback, and compressed with a 200000:1 ratio and -20db threshold.  The sine oscillators battle for a path through the extreme compression as they intermodulate.   The output is complex, and can be very sensitive to joystick input in some areas and insensitive in others.   Outputs   Mono sound is output to both channels of the line out and headphone sockets.   CARL approach   Reinforcement learning helps you to explore the complex soundscape of the doom compressor.   Files   Code   https://github.com/MusicallyEmbodiedML/MEMLNaut-DoomComp   Firmware Download   https://github.com/MusicallyEmbodiedML/MEMLNaut-DoomComp/releases/tag/v1.0   ","url": "/~ck84/meml/memlnaut/firmwares/doomcomppopr"
  },{
    "title": "MEMLNaut",
    "excerpt":"   MEMLNaut is a musical instrument and research platform for exploration of musically embodied machine learning.  It can process and generate sound, it’s extensible with a wide range of IO and interfaces, and it can optimise and run machine learning models on it’s dual core microcontroller chip.   MEMLNAut is open source, you can find all the Kicad files on our github.   ","url": "/~ck84/meml/memlnaut/"
  },{
    "title": "MEMLNaut Firmware: PAF Synth with POPR",
    "excerpt":"Overview   The PAF-POPR firmware offers exploration of Phase Aligned Formant synthesis, using the POPR approach to ML.   PAF   Phase Aligned Formant synthesis was developed my Miller Pucklette in the mid 90s.  It offers methods for creating complex spectra that avoid common issues with subtractive and FM.  The synth runs 3 PAF operators and a delay effect. 19 sound generation parameters are controlled by the three axes of the joystick.   Inputs   The PAF synth is controlled by MIDI. It listens for note on messages on channel 1.  These control the frequency of the first operator; the other two operators are detuned from this frequency.  The amplitude is also controlled by these messages.   Outputs   Mono sound is output to both channels of the line out and headphone sockets.   POPR approach   Use the POPR controls to try and build a new instrument: a joystick that controls sound by interpolating between the position-sound pairs that are contained in the training data.   Files   Code   https://github.com/MusicallyEmbodiedML/MEMLNaut-PAF-POPR   Firmware Download   https://github.com/MusicallyEmbodiedML/MEMLNaut-PAF-POPR/releases/tag/V1.0   ","url": "/~ck84/meml/memlnaut/firmwares/pafsynthpopr"
  },{
    "title": "MEMLNaut Firmware: PAF Synth with Reinforement Learning",
    "excerpt":"Overview   The PAF-CARL firmware offers exploration of Phase Aligned Formant synthesis, using the CARL approach to ML.   PAF   Phase Aligned Formant synthesis was developed my Miller Pucklette in the mid 90s.  It offers methods for creating complex spectra that avoid common issues with subtractive and FM.  The synth runs 3 PAF operators and a delay effect. 19 sound generation parameters are controlled by the three axes of the joystick.   Inputs   The PAF synth is controlled by MIDI. It listens for note on messages on channel 1.  These control the frequency of the first operator; the other two operators are detuned from this frequency.  The amplitude is also controlled ny these mnessages.   Outputs   Mono sound is output to both channels of the line out and headphone sockets.   Reinforcement Learning   When you indicate a reward, you are rewarding (negatively or positively) how a particular joystick position is mapped to the sound it’s making. As you indicate preferences for these settings, the DDPG algorithm will start to tune how the actor reacts to the joystick.   Files   Code   https://github.com/MusicallyEmbodiedML/MEMLNaut-PAF-CARL   Firmware Download   https://github.com/MusicallyEmbodiedML/MEMLNaut-PAF-CARL/releases/tag/v1.0   ","url": "/~ck84/meml/memlnaut/firmwares/pafsynthrl"
  },{
    "title": "ML Approaches: Perform-Optimise-Play-Repeat",
    "excerpt":"This approach to ML is conventionally refered to as Interactive Machine Learning, but we call it POPR instead to differentiate it from reinforcement learning approaches.  POPR describes an interactive, data-centric approach to machine learning, much like you would use in Fiebrink’s software Wekinator.  The musician works in a cycle of collecting data, editing the data into a training set (Perform), using this data to optimise or train a machine learing model, and then playing to evaluate the model in inference mode.  This loop might continue until a satisfying result has been achieved, or it may be an ongoing process.  At various points, the player might reset or cut the dataset, and may also reset (i.e. randomise) the model.   Quick-start guide      Plug the unit in, turn the joystick (or move the input sensors), hear the untrained model in action.   Flick Toggle B1 switch up to training mode.   Flick Momentary B1 switch up to randomise the network. Repeat as many times as desired.   When you find a sound that you want to keep, press the head button of the joystick, and keep it pressed. Move the joystick (or the input sensors) to a position where you would like the sound. Release the head of the joystick. You have created a data point in the dataset.   Repeat step 4 as many times as desired.   Flick Toggle B1 down to inference mode (centre position). Your network is now trained.   Controls   Momentary switches   Momentary B1 (up): randomise the weights of the network in training mode. Disabled in inference mode.   Momentary B2 (down): clear dataset. Wipes all data points in training mode. Disabled in inference mode.   Toggle switches   Toggle B1 (up): switch to training mode.   Toggle B2 (centre or down): switch to inference mode.   Toggle A1 (up): zoom in - takes the current sound and put it at the centre of the space, for finer exploration. Only in training mode, disabled in inference mode.   Toggle A2 (centre or down): zoom out - restores the full set of coordinates if previously in zoomed-in mode. Disabled in inference mode.   Pots   Z potentiometer: Sets the maximum number of iterations the network will be trained for.   Y potentiometer: Sets the zoom-in factor. Lower numbers make the zoom-in mode finer, higher number make it coarser (similar to the zoom-out mode). POPR.md 3 KB   ","url": "/~ck84/meml/memlnaut/approaches/popr"
  },{
    "title": "About",
    "excerpt":"MEML is funded by the UK Arts and Humanities Research Council. It is an investigation into the musically expressive potential of machine learning (ML) when embodied within physical musical instruments. It proposes ‘tuneable ML’, a novel approach to exploring the musicality of ML models, when they can be adjusted, personalised and remade, using the instrument as the interface.   ML has been highly successful in  allowing us to build novel creative tools for musicians. For example, generative models that bring new approaches to sound design, or models that allow musicians to build complex, nuanced mappings with musical gestures.  These instruments offer new forms of creative expression, because they are configurable in intuitive ways using data that can be created by musicians. They can also offer new modes of control, with techniques such as latent space manipulation. Currently, to train a ML model, standard practice is to collect data (e.g sound or sensor data), create and pre-test the model within a data science environment, before testing it with the instrument. This distributed approach creates a disconnection between the instrument and the machine learning processes. With ML embodied within an instrument, musicians will be able to take a more creative and intuitive approach to making and tuning models, that will also be more inclusive to those without expertise in ML. Musicians can get the most value from ML if the whole process of machine learning is accessible; there are many creative possibilities in the training and tuning of models, so it’s valuable if the musician can have access to the curation of data, curation of models, and to methods for ongoing retuning of models over their lifetime.   We have reached the point where ML technology will run on lightweight embedded hardware at rates sufficient for audio and sensor processing. This opens up innumerable additions to our electronic, digital, and hybrid augmented acoustic instruments.  Our instruments will contain lightweight embedded computers with ML models that shape key elements of the instruments behaviour, for example sound modification or gesture processing, responding to sensory input player and/or environment. This project will demonstrate how Tuneable ML creates novel musical possibilities, as it allows to create self-contained instruments, that can evolve independently from the complex data science tools conventionally used for ML.   The project asks how instruments can be designed to make effective and musical use of embedded ML processes, and questions the implications for instrument designers and musicians when tunable processes are a fundamental driver of an instrument’s musical feel and musical behaviour.   Team:   Chris Kiefer      Andrea Martelloni      Nic Seymour-Smith      Anna Thomas      MEML is support by Sussex Digtial Humanities Lab      ","url": "/~ck84/meml/about/"
  },{
    "title": "Posts by Category",
    "excerpt":" ","url": "/~ck84/meml/categories/"
  },{
    "title": "MEMLNaut Features",
    "excerpt":"The main MEMLNaut system features the following:      dual core RP2350B processor with extended IO   stereo audio I2S codec   mono microphone preamp   stereo headphone out   dual mono line ins and line outs   MIDI in and out (with MIDI DIN connectors)   SD Card socket   reset and boot select buttons   3D analogue Joystick   2 3-position toggle switches   2 2-way momentary toggle switches   a rotary encoder/switch   4 analogue potentiometers   a 320x240 colour TFT LCD screen   ","url": "/~ck84/meml/memlnaut/features/"
  },{
    "title": "MEMLNaut Firmware Guide",
    "excerpt":"There are several firmwares available, to configure the MEMLNaut in different modes: with varied approaches to signal processing and machine learning.   Each firmware is downloadable as a .uf2 file.  To install a firmware:      Connect the MEMLNaut to your computer.   Put the MEMLNaut into boot select mode: press reset while holding down the boot select button.   The microcontroller will now appear as a drive on your computer; drag the .uf2 file onto this drive.   The firmware will now upload, and the MEMLNaut will reboot with the new firmware running.   ","url": "/~ck84/meml/memlnaut/firmwares/guide"
  },{
    "title": "MEMLNaut Firmware: Multi-euclidean Sequencer with Reinforement Learning",
    "excerpt":"Overview   A bank of 8 euclidean sequencers are controlled by a neural network, mapped from 3 axes of the joystick.   Euclidean Sequencing   Euclidean sequencing describes a process that generates events according to two numbers: N and K.  N is the number of divisions within a time period.  K is the number of events the algorithm tries to fit into these divisions.  If N is divisible by K then a regularly spaced pattern is generated, otherwise the closest fit is chosen, creating varied rhythms.   A good analysis of euclidean rhythms can be found in Godfried Toussaint’s paper The Euclidean Algorithm Generates Traditional Musical Rhythms   Outputs   MIDI notes are output each time a euclidean generator triggers an new event.   Reinforcement Learning   When you indicate a reward, you are rewarding (negatively or positively) how a particular joystick position is mapped to the sequence it’s generating. As you indicate preferences for these settings, the DDPG algorithm will start to tune how the actor reacts to the joystick.   Files   Code   ","url": "/~ck84/meml/memlnaut/firmwares/multieuclideancarl"
  },{
    "title": "MEMLNaut Firmware: Multi FX Processor with Reinforement Learning",
    "excerpt":"Overview   This firmware offers exploration of multi fx, using the CARL approach to ML.   Multi Effect Algorithms   The processor has the following effects chain:   Pitch shifter -&gt; Stereo Delay -&gt; Dry/Wet Mix -&gt; Outputs   Output from the neural network controls:      The pitch shift frequency (from -12 to +12 semitones)   The delay time, feedback and dry/wet mix for delays on each channel   The overall dry/wet balance   Inputs   The processor takes a mono input on the right channel.   Outputs   Stereo sound is output to the line outs and headphone sockets.   Reinforcement Learning   When you indicate a reward, you are rewarding (negatively or positively) how a particular joystick position is mapped to the sound it’s making. As you indicate preferences for these settings, the DDPG algorithm will start to tune how the actor reacts to the joystick.   Files   Code   https://github.com/MusicallyEmbodiedML/memlnaut-multifx-carl   ","url": "/~ck84/meml/memlnaut/firmwares/multifxrl"
  },{
    "title": "MEMLNaut Firmware: Multi FX Processor with POPR",
    "excerpt":"Overview   This firmware offers exploration of multi fx, using the POPR approach to ML.   Multi Effect Algorithms   The processor has the following effects chain:   Pitch shifter -&gt; Stereo Delay -&gt; Dry/Wet Mix -&gt; Outputs   Output from the neural network controls:      The pitch shift frequency (from -12 to +12 semitones)   The delay time, feedback and dry/wet mix for delays on each channel   The overall dry/wet balance   Inputs   The processor takes a mono input on the right channel.   Outputs   Stereo sound is output to the line outs and headphone sockets.   POPR   Use the POPR approach to create joystick mappings that interpolate between multifx settings.   Files   Code   https://github.com/MusicallyEmbodiedML/memlnaut-multifx-popr   ","url": "/~ck84/meml/memlnaut/firmwares/multifxpopr"
  },{
    "title": "Posts by Tag",
    "excerpt":" ","url": "/~ck84/meml/tags/"
  },{
    "title": "MEMLNaut Technical Specification",
    "excerpt":"The MEMLNaut pcb is a RP2350B microcontroller with a stereo audio codec, MIDI IO and SD Card storage.  It provides pin headers to break out a wide range of IO from these systems.   PCB       Microcontroller      Raspberry Pi RP2350B   dual ARM M33 or RISCV cores   16Mb flash   pcb footprint for soldering another flash memory or a PSRAM chip   8 ADCs   range of GPIO options including SIO, UART, SPI, i2C, I2S, and custom IO with state machines   Audio CODEC      NXP SGTL5000 codec   stereo line out   stereo headphone output   stereo line in   mono microphone preamp with bias voltage   variety of sample rates   I2S   Power      runs from 5V USB power (USB-C connector)   separate 3.3V regulators for analogue and digital power domains   5V input for external power (non-USB)   Breakout Pinheaders      5V, 3,3V and 3.3V analog power   SWD   GPIO pins   ADCs   line ins and line outs   microphone input   SD Card SPI pins   I2S   I2C bus (with pull-up resistors)   vertical mount USB-C connector   Panels       Schematics        Github   https://github.com/MusicallyEmbodiedML/MEMLNaut   ","url": "/~ck84/meml/memlnaut/techspec/"
  },{
    "title": "Posts by Year",
    "excerpt":" ","url": "/~ck84/meml/posts/"
  }]
